---
share: true
---

# Standard Convolution

- Input
	1. Image Tensor: 维度 → B, C, H, W。H高，W宽，C通道数, B sample数。
	2. Conv Kernel: 维度 → N, N, D。N是长宽，D是深度（要求和C一样大，如果是图片作输入，C是3。）。一般会有K个Kernel；单个Kernel我们记为W；
- Process
	1. 在Image Tensor上提取出一块, I。跟Kernel的维度一致N, N, C；
	2. 把W和I展平，做点乘并加上bias：$s=I*W+b$。$s$是一个标量，代表Feature Map上的一个点；
	3. 滑动窗口：卷积核在输入图像上滑动，每次移动一定的步长（stride），通常是1个像素点。如果超过了输入图片的范围，则会进行padding，一般置0；
	4. 特征图反映了输入图像在卷积核探测的特定特征上的响应；
	5. 一个卷积层可能有K个不同的卷积核，每个都会生成一个不同的特征图。K一般被称为卷积层的通道数；
- Output
	- 特征图：维度 → B, K, (H - Stride), (W - Stride)。
	- 可以看出，特征图的通道数由卷积层通道数决定；

- Hyperparameter
	1. Stride：Kernel在卷积时移动的格子数；
	2. Padding

- 可学习参数：(NxNxD + 1)xK。每个卷积核NxNxD + 1；这一层有K个；

$\underline{优化实现}$

直接按照卷积的定义进行计算效率很低，一般采用所谓im2col的方法来提速，具体来说：
- Input
	1. 特征图F，维度 → C, H, W；
	2. 卷积核K，维度 → C, N, N；
- Process
	1. 把F和K都按im2col来转换，获得$F_{col}$和$K_{col}$；
	2. 实际所发生的是：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。  比如C×W×H大小的输入特征图，N×N大小的卷积核，输出大小为$C_o×W_o×H_o$，输入特征图将按需求被转换成$F_{col}=(N∗N)×(C∗W_o∗H_o)$的矩阵，卷积核将被转换成$K_{col}=C_o×(K∗K)$的矩阵；详见；
	2. 调用gemm来计算卷积结果；
- Property
	1. im2col本身需要花时间，且$F_{col}$和$K_{col}$相比原来占空间；
	2. 但是gemm计算很快；

# Group Convolution

## 基本原理

假设你有一个输入特征图，其通道数为C。在传统的卷积中，一个卷积核会跨越所有C个通道进行卷积。而在分组卷积中，你可以将输入特征图的通道分为G个组，每组包含C/G个通道。然后，你为每组分别应用卷积核，而不是让卷积核跨越所有的通道。这意味着每个卷积核只需要与其对应组内的通道进行卷积操作。

## Approach
1. 把输入的feature map按通道分成n组；
2. 把卷积核按通道分成n组；
3. 把分好的feature map和卷积核按组做卷积；
4. 得到n个输出feature map；

## Variants

- 当n = 1 → 普通卷积；
- 当n = n → depthwise separable conv，最后获得n个feature map；


## Application

- AlexNet：最初用于解决GPU内存限制问题，AlexNet的设计者将网络分成两部分在两个GPU上并行运行。
- ResNeXt：通过在ResNet的基础上使用分组卷积，ResNeXt提高了模型的准确度和效率。
- MobileNets：利用深度可分离卷积（depthwise separable convolution），一种特殊形式的分组卷积，它将每个通道作为一个组，显著减少了模型的复杂性。

## 优势

- 降低计算量：分组卷积减少了参数数量和计算量，因为每个卷积核只与部分输入通道进行卷积。具体来说，我们可以使用一样的参数量和运算量获得n倍的特征图输出；
- 并行化：不同组可以在不同的计算单元上并行处理，提高了计算效率。
- 提高表征能力：它强迫网络在较小的通道组内学习特征，可能增加了特征的多样性。

## 缺点

- 减少特征融合：由于卷积核不跨越所有通道，特征图之间的交互减少，可能会影响学习到的特征的质量。这个可以使用shuffle操作；
- 超参数增加：分组数量成为一个新的超参数，需要仔细选择以达到最佳的效果和效率平衡。

总的来说，分组卷积是一个有效的结构，可以在减少参数和计算成本的同时保持或甚至提高模型的性能。在设计卷积神经网络时，通过调整分组数目，可以针对特定的应用和硬件配置优化模型。

## Ref
[详解](https://www.jianshu.com/p/a936b7bc54e3)

# Transpose Convolution

转置卷积（Transpose Convolution），也称为分数步长卷积（Fractionally Strided Convolution）或逆卷积（Deconvolution），是一种常用于深度学习中的操作，尤其是在图像生成（如生成对抗网络GANs）和语义分割中。其基本目的是执行卷积操作的逆过程，从而将特征图的尺寸上采样（增大）。

## 工作原理

在正常的卷积中，我们将卷积核应用到输入特征图上，通常得到尺寸较小的输出特征图。相反，转置卷积通过在输入特征图的相邻元素之间插入空白（通常是零填充），然后应用卷积核，从而实现尺寸的增加。

## 数学描述

如果 $x$ 是输入特征图，$W$ 是卷积核，转置卷积 $y$ 可以表示为：

$$
y = W^T*x
$$

这里，$W^T$表示卷积核的转置（这里的转置不是数学上的矩阵转置，而是卷积核翻转180度之后执行的操作），∗ 表示卷积操作。转置卷积的过程包括卷积核与扩展后的输入特征图的每个区域的点积。

## 直观解释

- 考虑一个简单的一维例子，输入为[1,2][1,2]，卷积核为[1,0,−1][1,0,−1]。标准卷积的输出将是[1,2,−2][1,2,−2]。
- 转置卷积将首先在输入元素之间插入零，得到[1,0,2][1,0,2]，然后用卷积核执行正常卷积操作，输出可能是[1,−1,2,−2][1,−1,2,−2]，尺寸增加了。

## 应用

转置卷积通常用于以下情况：

- 图像生成：在GANs和变分自编码器（VAEs）中从低维潜在空间生成高分辨率图像。
- 特征图上采样：在语义分割和超分辨率等任务中，需要将低分辨率的特征图转换为高分辨率的输出。

## 注意事项

- 棋盘效应（Checkerboard Artifacts）：由于不均匀的重叠，转置卷积可能会在输出特征图中产生不希望的棋盘效应。为了避免这种情况，可以通过调整卷积核大小、步长和填充策略来设计转置卷积层。
- 学习参数：转置卷积层有自己的权重参数，这些参数通常是通过网络训练学习得到的。

转置卷积是一种强大的工具，使得网络能够学习如何从压缩的特征表示中恢复出详细的空间信息，但它的使用需要仔细设计以避免引入伪影。

# Activation

在卷积层之后通常会跟随一个激活函数，它的目的是为网络引入非线性，因为卷积操作本身是线性的。最常见的激活函数是：

1. ReLU（Rectified Linear Unit）: 快速而高效，将所有负值设置为0。
2. Leaky ReLU: 允许负值有一个小的、固定的正梯度。
3. Parametric ReLU: Leaky ReLU的变体，其正梯度是可学习的参数。
4. Sigmoid: 将输入压缩到0和1之间，用于二分类问题。$\sigma(x) = \frac{1}{1 + e^{-x}}$。
	- 优点：平滑梯度，输出范围在 0 到 1 之间，非常适合用作输出层，特别是在二分类问题中；
	- 缺点：梯度消失问题 → 在两端饱和区域，梯度几乎为零，这使得深层网络中的梯度更新非常缓慢。计算资源消耗也比较大；
6. Tanh: 将输入压缩到-1和1之间，中心化数据；但是会有梯度消失的问题；
7. SiLU：Sigmoid Linear Unit。也称为Swish函数，SiLU的形式是将输入值乘以sigmoid函数的输出。它的特点是：
	- 它能够在负输入值时产生小于零的输出，这允许神经元有更大的表示范围；
	- 在正输入值时，它能够防止激活值增长过快；
	- 它在正数部分是非饱和的，这意味着对于大的正输入值，其导数不会趋近于零，减少梯度消失的风险；

选择哪种激活函数取决于具体任务和网络架构。ReLU因为计算简单和效果良好，是最常用的激活函数。

# Pooling

Pooling操作是卷积神经网络中常用的一种下采样技术，它的目的是减小特征图的维度，提高网络的计算效率，并增强特征的抗干扰能力。常见的Pooling操作有：

1. Max Pooling：选取覆盖区域内的最大值作为该区域的代表。
2. Average Pooling：计算覆盖区域内所有值的平均值。
3. Global Pooling：在整个特征图上进行Pooling，每个特征图只输出一个值。

Pooling操作通过减少每层的参数数量，有助于防止过拟合，并且允许网络在图像内部的小的位置变化中保持特征不变性。