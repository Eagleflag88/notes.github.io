---
share: true
---

# Standard Convolution

- Input
	1. Image Tensor: 维度 → B, C, H, W。H高，W宽，C通道数, B, sample数。
	2. Conv Kernel: 维度 → N, N, D。N是长宽，D是深度（要求和C一样大，如果是图片作输入，C是3。）。一般会有K个Kernel；单个Kernel我们记为W；
- Process
	1. 在Image Tensor上提取出一块, I。跟Kernel的维度一致N, N, C；
	2. 把W和I展平，做点乘并加上bias：$s=I*W+b$。$s$是一个标量，代表Feature Map上的一个点；
	3. 滑动窗口：卷积核在输入图像上滑动，每次移动一定的步长（stride），通常是1个像素点。如果超过了输入图片的范围，则会进行padding，一般置0；
	4. 特征图反映了输入图像在卷积核探测的特定特征上的响应；
	5. 一个卷积层可能有K个不同的卷积核，每个都会生成一个不同的特征图。K一般被称为卷积层的通道数；
- Output
	- 特征图：维度 → B, K, (H - N + 2xPadding)/Stride, (W - N + 2xPadding)/Stride。
	- 可以看出，特征图的通道数由卷积层通道数决定；

- Hyperparameter
	1. Stride：Kernel在卷积时移动的格子数；
	2. Padding

- 可学习参数：(NxNxD + 1)xK。每个卷积核NxNxD + 1，这一个卷积层有K个；

$\underline{优化实现}$

直接按照卷积的定义进行计算效率很低，一般采用所谓im2col的方法来提速，具体来说：
- Input
	1. 特征图F，维度 → C, H, W；
	2. 卷积核K，维度 → C, N, N；
- Process
	1. 把F和K都按im2col来转换，获得$F_{col}$和$K_{col}$；
	2. 实际所发生的是：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。  比如C×W×H大小的输入特征图，N×N大小的卷积核，输出大小为$C_o×W_o×H_o$，输入特征图将按需求被转换成$F_{col}=(N∗N)×(C∗W_o∗H_o)$的矩阵，卷积核将被转换成$K_{col}=C_o×(K∗K)$的矩阵；
	2. 调用gemm来计算卷积结果；
- Property
	1. im2col本身需要花时间，且$F_{col}$和$K_{col}$相比原来占空间；
	2. 但是gemm计算很快；
- 具体细节可以参考[Im2Col](https://zhuanlan.zhihu.com/p/70703846)

# Group Convolution

## 基本原理

假设你有一个输入特征图，其通道数为C。在传统的卷积中，一个卷积核会跨越所有C个通道进行卷积。而在分组卷积中，你可以将输入特征图的通道分为G个组，每组包含C/G个通道。然后，你为每组分别应用卷积核，而不是让卷积核跨越所有的通道。这意味着每个卷积核只需要与其对应组内的通道进行卷积操作。

## Approach
1. 把输入的feature map按通道分成n组；
2. 把卷积核按通道分成n组；
3. 把分好的feature map和卷积核按组做卷积；
4. 得到n个输出feature map；

## Variants

- 当n = 1 → 普通卷积；
- 当n = n → depthwise separable conv，最后获得n个feature map；


## Application

- AlexNet：最初用于解决GPU内存限制问题，AlexNet的设计者将网络分成两部分在两个GPU上并行运行。
- ResNeXt：通过在ResNet的基础上使用分组卷积，ResNeXt提高了模型的准确度和效率。
- MobileNets：利用深度可分离卷积（depthwise separable convolution），一种特殊形式的分组卷积，它将每个通道作为一个组，显著减少了模型的复杂性。

## 优势

- 降低计算量：分组卷积减少了参数数量和计算量，因为每个卷积核只与部分输入通道进行卷积。具体来说，我们可以使用一样的参数量和运算量获得n倍的特征图输出；
- 并行化：不同组可以在不同的计算单元上并行处理，提高了计算效率。
- 提高表征能力：它强迫网络在较小的通道组内学习特征，可能增加了特征的多样性。

## 缺点

- 减少特征融合：由于卷积核不跨越所有通道，特征图之间的交互减少，可能会影响学习到的特征的质量。这个可以使用shuffle操作；
- 超参数增加：分组数量成为一个新的超参数，需要仔细选择以达到最佳的效果和效率平衡。

总的来说，分组卷积是一个有效的结构，可以在减少参数和计算成本的同时保持或甚至提高模型的性能。在设计卷积神经网络时，通过调整分组数目，可以针对特定的应用和硬件配置优化模型。

## Ref
[详解](https://www.jianshu.com/p/a936b7bc54e3)

# Transpose Convolution

转置卷积（Transpose Convolution），也称为分数步长卷积（Fractionally Strided Convolution）或逆卷积（Deconvolution），是一种常用于深度学习中的操作，尤其是在图像生成（如生成对抗网络GANs）和语义分割中。其基本目的是执行卷积操作的逆过程，从而将特征图的尺寸上采样（增大）。

## 工作原理

在正常的卷积中，我们将卷积核应用到输入特征图上，通常得到尺寸较小的输出特征图。相反，转置卷积通过在输入特征图的相邻元素之间插入空白（通常是零填充），然后应用卷积核，从而实现尺寸的增加。

## 数学描述

如果 $x$ 是输入特征图，$W$ 是卷积核，转置卷积 $y$ 可以表示为：

$$
y = W^T*x
$$

这里，$W^T$表示卷积核的转置（这里的转置不是数学上的矩阵转置，而是卷积核翻转180度之后执行的操作），∗ 表示卷积操作。转置卷积的过程包括卷积核与扩展后的输入特征图的每个区域的点积。

## 直观解释

- 考虑一个简单的一维例子，输入为[1,2][1,2]，卷积核为[1,0,−1][1,0,−1]。标准卷积的输出将是[1,2,−2][1,2,−2]。
- 转置卷积将首先在输入元素之间插入零，得到[1,0,2][1,0,2]，然后用卷积核执行正常卷积操作，输出可能是[1,−1,2,−2][1,−1,2,−2]，尺寸增加了。

## 应用

转置卷积通常用于以下情况：

- 图像生成：在GANs和变分自编码器（VAEs）中从低维潜在空间生成高分辨率图像。
- 特征图上采样：在语义分割和超分辨率等任务中，需要将低分辨率的特征图转换为高分辨率的输出。

## 注意事项

- 棋盘效应（Checkerboard Artifacts）：由于不均匀的重叠，转置卷积可能会在输出特征图中产生不希望的棋盘效应。为了避免这种情况，可以通过调整卷积核大小、步长和填充策略来设计转置卷积层。
- 学习参数：转置卷积层有自己的权重参数，这些参数通常是通过网络训练学习得到的。

转置卷积是一种强大的工具，使得网络能够学习如何从压缩的特征表示中恢复出详细的空间信息，但它的使用需要仔细设计以避免引入伪影。

# Inductive Biases

在深度学习和计算机视觉的语境中，当我们谈论卷积神经网络（CNN）的 "inductive biases"（归纳偏置）时，我们指的是那些内嵌于网络架构中的假设和约束，这些假设和约束影响了模型学习数据的方式。这些归纳偏置并非来自数据本身，而是由网络的结构和其运行机制决定的。对于 CNN 来说，主要的归纳偏置包括：

1. 局部性（Local Connectivity）：每个卷积操作涉及输入数据的一个局部区域，而不是全局。这意味着网络在学习特征时关注输入数据的局部区域（例如，图像中的小块区域），从而捕获局部特征。
2. 权重共享（Weight Sharing）：在整个网络中，同一个卷积核（包含一组权重）被应用到输入数据的不同部分。这种权重共享机制减少了模型的参数数量，增强了网络对于相似模式的检测能力，无论这些模式出现在输入数据的哪个位置。
3. 平移不变性（Translation Invariance）：由于权重共享和池化层（例如最大池化），CNN 能够在图像中不同位置检测到相似的模式。换句话说，无论特征在图像中的哪个位置，网络都能识别出它，这导致了对平移的不变性。
4. 层级结构（Hierarchical Structure）：CNN 通常有多层结构，较低层次的网络部分学习更简单、更具有普遍性的特征（如边缘和角点），而较高层次则学习更复杂、更抽象的特征（如对象的部分和整体结构）。

这些归纳偏置使得 CNN 特别适合处理图像数据，因为这些偏置反映了自然图像的常见特性，例如局部区域的相关性和对象在视觉场景中的层级组织。同时，这些偏置也限定了 CNN 的适用范围和性能，例如它们可能不是处理非图像类数据的最佳选择。