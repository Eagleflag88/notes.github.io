---
share: true
---
# Preprocessing

## Normalization
- Whitening：让数据的协方差矩阵为单位矩阵；
- PCA：让数据的协方差矩阵只有对角线元素非0；
- Best Practice：一般只会center；

## Dataset Partition
- 分为training, validation, test三个数据集；
- 利用training和validation做cross validation获得超参数；
- 最后在test集上跑一次，获得结果；

# Weight Initialization

权重初始化在深度学习模型的训练中非常关键，它可以显著影响模型的收敛速度和最终性能。以下是一些权重初始化的关键点：

1. 避免权重初始值过大或过小：权重过大可能导致神经元饱和，梯度消失或爆炸；权重过小也可能导致梯度消失；
2. 随机初始化：通常权重是随机初始化的，以打破对称性并确保不同的神经元可以学习不同的特征；
3. 基于输入和输出尺寸的初始化（如Xavier/Glorot初始化）：适用于tanh激活函数，初始化时考虑到前一层的神经元数量，以保持激活和梯度的分布稳定。它的结果是一个以0为均值的高斯分布，$D_{in}$是某一层的输入维度：
	$$
	W = \mathcal{N}(0, \sqrt{\frac{1}{D_{in}}})
  $$
4. He初始化：特别为ReLU激活函数设计，考虑到ReLU在负值区域的神经元不活跃的特性，因此初始权重需要更大。它的结果是：
	$$
	W = \mathcal{N}(0, \sqrt{\frac{2}{D_{in}}})
  $$

权重初始化的选择应考虑到所使用的激活函数、网络架构和任务的特性。正确的初始化方法可以确保梯度的良好流动，防止训练初期的梯度问题，并有助于模型更快地收敛。

# Regularization

在深度学习训练中，正则化技术用于防止模型过拟合，从而提高模型在新数据上的泛化能力。以下是一些常用的正则化技术：

1. 权重衰减（L2正则化）：又叫Weight Decay。通过在损失函数中添加一个与权重大小成比例的项来惩罚大的权重值，通常表现为所有权重的平方和乘以一个常数。
2. L1正则化：与L2类似，但是惩罚项是权重的绝对值之和，这可以导致稀疏的权重矩阵，有些权重会变成0。  
3. Dropout：在训练过程中随机丢弃（即设置为零）网络中的一些神经元输出，以减少神经元之间复杂的共适应。
	- 在训练的每次迭代中，每个神经元都有一定概率（通常设为 0.5）被随机丢弃，即它在前向传播和反向传播时暂时不参与计算；
	- 实际上等同于每次都在训练不同的网络；
	- 由于在训练时某些神经元被丢弃，因此在预测时需要对神经元的输出进行缩放，以补偿那些在训练时未被激活的神经元。如果在训练时使用了p的丢弃概率，则在预测时，网络的权重通常会乘以1−p（这个过程有时也被称为"inverted dropout"）；
	- Dropout一般在卷积过后的全连接层使用；
1. 数据增强：通过对训练数据进行旋转、缩放、裁剪等变换来人为增加样本多样性，提高模型的泛化能力。
2. 早停（Early Stopping）：在验证集上的性能不再提升时停止训练，防止过拟合。
3. Normalization：BN，GN和LN。详见[[Normalization#Batch Normalization|Normalization > Batch Normalization]]，[[Normalization#Group Normalization|Normalization > Group Normalization]]和[[Normalization#Layer Normalization|Normalization > Layer Normalization]]；
4. 梯度剪切（Gradient Clipping）：限制梯度更新的步长，防止梯度爆炸；
5. 学习率衰减：随着时间的推移逐渐减小学习率，有助于模型在训练后期稳定；

这些技术可以单独使用，也可以组合使用，以达到最好的正则化效果。选择哪种技术通常取决于具体任务、模型复杂性和训练数据的特性。

# Optimization

## 一阶优化算法
- Gradient Steepest：
	1. 直接朝gradient的反方向走；
	2. “最陡”方向可以根据不同的范数来定义。例如，在欧几里得范数下，最陡下降方向与梯度下降相同；
- Gradient Descent：
	1. Zig-Zag Behavior because of poor conditioning；
	2. Get stuck in local minima or saddle points；
-  Stochastic Gradient Descent（SGD）：
	1. 小范围应用Batch Gradient Descent；
	2. 超参数batch size，一般选32；
-  SGD+Momentum：
	1. Get over with saddle points with velocity (initial 0)；
	2. Nesterov Momentum，它不是在当前位置计算梯度，而是在当前动量方向上提前一步的位置计算梯度；
- AdaGrad：
	1. 缓解各方向梯度不均匀的问题；
	2. Approach：主要特点是在训练过程中累积了一个参数的历史梯度的平方和，然后用这个信息来调整每个参数的学习率；
	3. Issue：时间久了，增量容易变为0；
- RMSProp-AdaGrad：添加一个decay rate来缓解增量变0的问题；
- Adam：Combination of SGD-Momentum and RMSProp；

# 	Learning Rate

在深度学习训练中，学习率是控制模型权重调整速度的一个关键超参数。学习率过高可能导致训练不稳定，甚至发散；学习率过低则可能导致训练速度过慢，或陷入局部最小值。因此，正确调整学习率对于有效训练深度神经网络至关重要。以下是一些调整学习率的常用策略：

1. 预设减小学习率：事先设定一个学习率调整计划，随着训练的进行逐步降低学习率。例如，可以每过一定数量的epochs将学习率减小一半。
2. 性能调度：观察模型在验证集上的性能，如果性能停止提高，则降低学习率。
3. 指数或者步进衰减：按照指数函数逐渐减小学习率，或者在固定的训练步数后降低学习率。
4. 适应性学习率算法：例如Adagrad, RMSprop, 和Adam等，这些算法能够自适应地调整每个参数的学习率，通常不需要手动调整。
5. 周期性调整：如余弦退火学习率（Cosine Annealing），在每个周期内逐渐降低学习率，然后重置到较高的值。
6. 热启动重启：这是周期性调整的一种变体，每次重启时将学习率设置到较大的值，允许模型跳出局部最小值。    

调整学习率的最佳方法可能依赖于具体的模型、数据集和训练设置。在实践中，通常需要结合使用上述几种策略，并可能需要多次尝试和交叉验证来确定最优的学习率调整策略。


# 连乘效应
在神经网络的反向传播过程中，连乘效应指的是梯度在多层网络中传递时的累积乘积。由于深度学习模型通常由多层组成，梯度必须通过这些层反向传播以更新权重。每一层的梯度是由该层的激活函数导数和来自上一层的梯度的乘积计算得出的。这个连乘过程可以导致两个主要问题：梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Exploding）。

## 梯度消失：

当网络很深时，梯度在经过多个层的反向传播过程中可能会越来越小，最终变得非常微小，使得权重更新非常缓慢或者几乎不更新。这种现象称为梯度消失。尤其是当使用像Sigmoid或Tanh这样的饱和激活函数时，它们的导数在输入值非常大或非常小的时候会接近于零，导致梯度消失问题更加严重。

## 梯度爆炸：

与梯度消失相反，梯度爆炸是指梯度在经过多个层的反向传播时变得越来越大。这会导致权重更新过大，使得网络权重波动非常剧烈，甚至导致模型发散。在实际操作中，梯度爆炸通常通过梯度剪切（Gradient Clipping）来缓解。

## 解决方法：

1. 非饱和激活函数：使用像ReLU及其变体这样的非饱和激活函数可以缓解梯度消失问题。
2. 合适的权重初始化：如He初始化或Xavier初始化，它们可以帮助在训练初期保持梯度的规模。
3. 批量归一化（Batch Normalization）：可以减少训练过程中各层输入分布的改变，有助于缓解梯度消失问题。
4. 残差连接（Residual Connections）：如在ResNet中使用的跳过连接，允许梯度直接流向前面的层，可以减轻梯度消失。
5. 梯度剪切：在反向传播过程中限制梯度的最大值，以防止梯度爆炸。

连乘效应是深度神经网络设计和训练中需要特别注意的问题，正确理解和处理这个问题对于训练稳定和有效的模型至关重要。






