---
share: true
---
# Batch Normalization

## Concept
- Batch Normalization（批量归一化）是一种在深度神经网络中常用的技术，用以加速训练过程并提高模型的稳定性。其核心思想是在网络的每个批量数据通过激活函数之前，先对该层的输入进行归一化处理，即将输入数据调整为均值为0、方差为1的分布；
- 这样做可以减少所谓的“内部协变量偏移”（internal covariate shift），并允许使用更高的学习率以及减少模型对初始化权重的敏感度；

## Process
BN在每个mini-batch的数据通过以下步骤进行：

1. 计算均值：计算当前mini-batch的均值。
2. 计算方差：计算当前mini-batch的方差；
3. 这里的计算范围是特征图的某一个通道，比如说特征图的维度是50x60x8，那么均值是在某一个通道的特征图层内计算的，即是计算比如说50x60个数的均值和方差；如果batch_size=8，则是计算50x60x8个数的均值和方差；
4. 归一化：使用上述均值和方差对输入数据进行归一化处理，确保输出数据的均值为0，方差为1；
5. 缩放和偏移：对归一化后的数据进行缩放和偏移变换，这里引入了两个可学习参数，即γ（缩放因子）和β（偏移因子）；

In Training using the mean and variance of the mini batches, at inference the mean and variance of the whole training set will be used.

## 可学习参数
Batch Normalization 有两组可学习参数：

1. γ（Gamma）: 缩放参数，用于对归一化后的数据进行缩放。
2. β（Beta）: 偏移参数，用于对归一化后的数据进行平移。
3. 每个通道会有一组参数；

# Batch ReNormalization

Batch ReNormalization 是 Batch Normalization 的一个变种，它旨在解决 Batch Normalization 在小批量数据上应用时效果不佳的问题。Batch ReNormalization 引入了两个新的参数，用于调整每个 mini-batch 的均值和方差，以便更好地近似整个数据集的统计特性。这使得它在使用小批量或非独立同分布（non-i.i.d.）数据时更为稳健。在 Batch ReNormalization 中，mini-batch 的均值和方差会被调整以考虑整个数据集的均值和方差，从而减少在训练不同阶段由于批量大小不一致引起的内部协变量偏移。


# Group Normalization

GN是一种在神经网络中使用的方法，尤其是在深度学习模型中，用来稳定学习过程。它是BN的替代方法，特别适用于小批量尺寸的训练。

BN是在批量维度上对特征图进行归一化，而GN通过将通道分成组，在这些组内计算用于归一化的均值和方差。这使得GN能够独立于批量大小操作，因此在由于内存限制而不能依赖大批量大小的情况下特别有优势。

GN背后的关键概念包括：

1. 通道分组：特征图的通道被划分为多个组。组的数量是一个可以调整的超参数。如果组的数量设置为通道数，GN则与实例归一化（Instance Normalization）类似。
2. 归一化：GN对每个组计算归一化的均值和方差。归一化是按通道进行的，它是对每个单独样本应用的，而不是跨批量。即使在batch size大于1的情况下也不会跨Batch；
3. 缩放和位移参数：归一化之后，GN应用可学习的缩放和位移参数来保持网络的表示能力。这些参数在训练过程中学习。
4. 独立于批量大小：由于GN在通道组内计算统计数据，它不需要大批量大小。这对于需要高分辨率输入图像的任务或当可用内存有限时特别有用。
5. 训练稳定性：GN可以导致深度学习模型的训练更加稳定，特别是在BN可能不适用的小批量大小或需要跨不同样本一致归一化的任务中。

GN在各种任务中得到应用，如图像识别、目标检测和语义分割，并且在批量大小受到硬件限制的场景中特别有益。如Faster R-CNN、Mask R-CNN和其他3D任务，由于计算和内存限制，可能无法使用较大的批量。GN在这些模型中可以提供有效的归一化，而不受批量大小的影响；

# Layer Normalization
详见[[Transformer#Layer Normalization|Transformer > Layer Normalization]]

# Activation

在卷积层之后通常会跟随一个激活函数，它的目的是为网络引入非线性，因为卷积操作本身是线性的。最常见的激活函数是：

1. ReLU（Rectified Linear Unit）: 快速而高效，将所有负值设置为0。
2. Leaky ReLU: 允许负值有一个小的、固定的正梯度。
3. Parametric ReLU: Leaky ReLU的变体，其正梯度是可学习的参数。
4. Sigmoid: 将输入压缩到0和1之间，用于二分类问题。$\sigma(x) = \frac{1}{1 + e^{-x}}$。
	- 优点：平滑梯度，输出范围在 0 到 1 之间，非常适合用作输出层，特别是在二分类问题中；
	- 缺点：梯度消失问题 → 在两端饱和区域，梯度几乎为零，这使得深层网络中的梯度更新非常缓慢。计算资源消耗也比较大；
5. Tanh: 将输入压缩到-1和1之间，中心化数据；但是会有梯度消失的问题；
6. SiLU：Sigmoid Linear Unit。也称为Swish函数，SiLU的形式是将输入值乘以sigmoid函数的输出。它的特点是：
	- 它能够在负输入值时产生小于零的输出，这允许神经元有更大的表示范围；
	- 在正输入值时，它能够防止激活值增长过快；
	- 它在正数部分是非饱和的，这意味着对于大的正输入值，其导数不会趋近于零，减少梯度消失的风险；
7. GeLU：高斯误差线性单元（Gaussian Error Linear Unit）。它的数学表达式是：$GeLU(x)=x\Phi(x)$，其中$\Phi(x)$是输入x的标准正态分布的累积分布函数（CDF）：
	- 结合了Sigmoid函数和线性单元的属性，使得它可以根据输入有选择地进行门控，这类似于ReLU（Rectified Linear Unit）激活函数的随机版本；
	- BERT是第一个使用它的重要模型；

选择哪种激活函数取决于具体任务和网络架构。ReLU因为计算简单和效果良好，是最常用的激活函数。

# Pooling

Pooling操作是卷积神经网络中常用的一种下采样技术，它的目的是减小特征图的维度，提高网络的计算效率，并增强特征的抗干扰能力。常见的Pooling操作有：

1. Max Pooling：选取覆盖区域内的最大值作为该区域的代表。
2. Average Pooling：计算覆盖区域内所有值的平均值。
3. Global Pooling：在整个特征图上进行Pooling，每个特征图只输出一个值。

Pooling操作通过减少每层的参数数量，有助于防止过拟合，并且允许网络在图像内部的小的位置变化中保持特征不变性。





