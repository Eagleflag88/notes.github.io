---
share: true
---

# Embedding Layer
- 嵌入层通常用于将离散的词索引映射到连续的向量空间。它不执行传统的矩阵乘法，而是作为一个查找表，将每个离散索引直接映射到预先定义的或学习到的向量；
- 具体来说，学习出来的嵌入层包含一个嵌入矩阵，其维度是$V \times D$，$V$是词汇表的大小，$D$是嵌入维度。每个行对应于词汇表中的一个词。当输入一个词的索引时，嵌入层返回该索引对应的行。这个过程可以看作是一个特殊的线性变换，没有偏置项，并且权重矩阵是稀疏的（只有一个非零元素）；
- 如果嵌入层的输入维度是$L \times 1$，则经过查询后的输出维度是$L \times D$；

# Positional Encoding

- 仅仅只有token的embedding是不能体现输入的全部特征的，比如说输入中各个token的位置关系，特别是在NLP领域中。

## Potential Solutions
- 方法一：直接用token的index，会有数值上的问题；
- 方法二：Normalized过后的index，不同长度的序列的步长是不一致的；
- 方法三：利用二进制向量表述位置信息，离散的向量，无法训练；

好的方法需要满足**有界**且**连续**的。

## Approach
- 假设$t$是这个token在序列中的实际位置（例如第一个token为1，第二个token为2...）；  
-  定义$PE_t$是是这个token的位置向量，长度为$d$。 $PE_{t}^{(i)}$ 表示这个位置向量里的第i个元素；
- $d_{model}$ 是这个token的维度（在论文中，是512)；

则$PE_{t}^{(i)}$可以表示为：  
$$
PE_{t}^{(i)}=sin⁡(w_it),\ if\ k=2i 
$$
$$
PE_{t}^{(i)}=cos⁡(w_it),\ if\ k=2i+1 
$$

这里：  
$$i w_i = \frac{1}{10000^{2i/d_{model}}} $$   
$$ i = 0,1,2,3,...,\frac{d_{model}}{2} -1 $$

## 性质
- 两个位置编码$PE_{t}^{(i)}$和$PE_{t}^{(j)}$的点积(dot product)仅取决于偏移量，也即两个位置编码的点积可以反应出两个位置编码间的距离；
- 位置编码的点积是无向的；

## Relative Potisional Embedding
上面介绍的都是按照token绝对位置编码的方法，事实上，我们也可以利用token之间的相对位置进行位置编码，大概步骤如下：
1. 首先计算token之间的相对位置。例如，如果词 A 在词 B 的前面，且两者之间有3个词，那么相对位置就是 -3；
2. Relative Positional Embedding：为每个可能的相对位置分配一个嵌入向量。这些嵌入可以是可学习的，或者根据某种预定义的函数生成；
3. 嵌入应用：将相对位置嵌入应用于模型的输入。通常，这些嵌入会与词汇的嵌入向量一起被输入到模型中，可以通过加法或拼接（concatenation）的方式结合它们；

## Relative Position Bias
- Relative Position Bias旨在通过向注意力机制引入偏差来帮助模型偏向于关注与当前位置相对位置特定的其他位置；
- 通常通过在注意力分数计算中添加一个可学习的偏差项$b$来实现，该偏差项基于元素之间的相对位置。公式如下：

$$ 
Attention(Q,K,V) = \sum^L (a + b)V
$$


# Attention

## Concept
1. 从众多关注点中找到最重要的关注点；
2. 聚焦的过程体现在**权重系数**的计算上，权重越大越聚焦于其对应的**value**值上。即权重代表了信息的重要性，而**value**是其对应的信息；
3. 比如在翻译一个句子的某个词的时候，我们不应该把注意力平均放到所有的输入上面，而是应该聚焦到某个输入，思考他应该如何被翻译。而具体应该聚焦到输入的哪个部分则是训练之后学习到的；  

## Process

$\underline{输入}$
- $Q$：Query，指询问，是系统实际的输入。
- $K$和$V$：键值对。


$\underline{计算过程}$
1. 计算$Q$对每个$K$的相似性，可以用向量点积（$K*Q$），或者其他任何相似性的表达方式。其结果是注意力得分$S$；
2. 对这一批得分进行softmax操作，归一化，获得一个概率分布，描述的是每个$K$的权重系数$a$；
3. 根据权重系数对$V$进行加权求和，即可求出Query在现在这个$K,Q$组合的Attention数值：
$$ 
Attention(Q,K,V) = \sum^L aV
$$

## Padding Mask
- Padding mask是用来处理变长序列的机制。由于神经网络通常需要固定长度的输入，但实际应用中的序列数据往往是变长的，因此通常会通过填充（padding）的方式来使所有序列达到同一长度；
- 但是，填充的部分不应该对模型的计算产生影响，这就需要一种机制来告诉模型哪些位置是填充的，应该被忽略。这种机制就是通过padding mask来实现的；
- 实现：
	1. Padding mask是一个与输入序列同形状的布尔掩码（Boolean mask），在该掩码中，填充位置的值为True，而非填充位置的值为False；
	2. Padding的机制在所有的$Q$，$K$和$V$上都会实施。在计算注意力分数后，我们在softmax函数应用之前对这些被标记为padding的位置施加极小值（例如，将这些位置的分数设置为负无限大）；
	3. Softmax函数会将这些极小值转换成接近零的概率，从而有效地在计算最终的注意力加权和时排除这些位置；

## Self-Attention

一种特殊的Attention机制，$Q$和$K$是同一个向量。

$\underline{输入}$
- $X$: 维度是$L \times D$。$L$是token的个数，$D$是一个超参数，可以定为768。实际上是输入进行embedding之后的结果。
- $W_Q$,$W_K$和$W_V$：维度是$D \times D$，用来对$X$进行线性变换，可学习的参数。

$\underline{计算过程}$
1. 对$X$进行线性变换，如下：
$$ 
Q = W_QX
$$
$$ 
K = W_KX
$$
$$ 
V = W_VX
$$
获得，$Q$，$K$和$V$，他们都是$L \times D$维的。$Q$是我们正要查询的信息，$K$是正在被查询的信息，$V$是被查询到的内容
2. 计算$Q$和$K$的相似性，注意，这里的计算具体来说是在$Q_i$和$K_i$，$V_i$之间进行的。$i$代表第$i$行。每一行都代表一个token的相关向量。然后利用softmax进行归一化，获得注意力权重$A$，维度是$L \times L$；
3. 计算注意力：$Z = A*V$，维度是$L \times D$；
4. 把注意力除以$\sqrt{D}$，其目的是把结果方差从$D$控制回1，有效控制梯度消失问题；
5. 总结下来，计算的公式可以总结为：
$$
Attention(Q, K, V) = softmax (\frac{QK^T}{\sqrt{D}})V
$$

## Multi-Head Attention
- 多头注意力机制，是在自注意力的基础上，使用多种变换生成的$Q$，$K$，$V$进行计算，再将它们对相关性的结论综合起来，进一步增强自注意力的效果。
- 多头指的就是多套$Q$，$K$，$V$。比如论文原文是8个头，那就是8套$Q$，$K$，$V$。
- 经过注意力之后的矩阵会有自己理解的语义信息，那所以最后8个$Z$就会有8个不同的理解。
- 多头信息输出，由于多套参数得到了多个信息，然而我们还是只需要一个信息，因此可以通过某种方法(例如矩阵相乘)把多个信息汇总为一个信息。
- 实际操作时，只会用一套$W_Q$,$W_K$和$W_V$，然后把生成的$Q$，$K$和$V$切成几个部分，进行后面的处理。

## Layer Normalization
- 核心思想是对单个样本中的所有激活值进行规范化，以稳定学习过程；
- 与BatchNorm相比：在一个Batch之内包含若干个sample，每个sample有若干个feature，BN是在feature层面进行归一化，LayerNorm是在每个sample内部跨feature做归一化；
- BN依赖于整个批次的数据来计算均值和方差，这意味着它的效果受批次大小的影响。层归一化在单个样本级别上进行，因此它不受批次大小或者批次间变异的影响；
- LN适用于不同的结构：在循环神经网络（RNNs）和一些其他不规则的网络结构中，BN可能难以有效实施，因为网络的时间步长或者结构上的特异性可能导致批次间的不一致性。LN因为是在单个样本上操作，所以能够更自然地适应这些结构。

$\underline{计算过程}$
1. 计算某一层输出的均值和方差；这里方差和均值的统计范围是整个特征图（不区分通道）。如果输入是50x60x8，我们将计算整个60x50x8特征图的全局均值和方差；
2. 使用上述统计量对每个激活值进行规范化；
3. 对规范化后的值进行缩放和平移；这里的可学习参数只有两个，跟通道数无关；

## Feed-forward Network

两层的全连接加上中间的激活函数。通过激活函数引入非线性变换，变换了Attention output的空间, 从而增加了模型的表现能力。

## Training

Attention中可学习的参数有：
1. Token的Embedding；
2. 对输入的线性变换$W_Q$,$W_K$和$W_V$；
3. Positional Embedding；
4. LayerNorm的平移和缩放参数；
5. FFN网络的参数；


# Transformer-Encoder

## Structure
Encoder是由多个一样的结构堆叠而成，每个结构由下列模块组成：

1. 输入嵌入（Input Embedding）：输入序列首先被转换成固定大小的向量，这一过程通过嵌入层完成。这些嵌入还会与位置编码（Positional Encoding）相结合（一般就是相加），以保留序列中单词的顺序信息；
2. 多头自注意力（Multi-Head Self-Attention）：在自注意力机制中，每个输入向量会转换成三个不同的向量，分别为查询（Query）、键（Key）和值（Value）。通过这三个向量的交互，模型能够确定输入序列中的每个元素应该给予其他元素多少“注意力”；
3. 残差连接和层归一化（Residual Connection & Layer Normalization）：自注意力层的输出通过一个残差连接，然后进行层归一化。残差连接允许直接的梯度流，有助于训练深层网络。
4. FFN：自注意力机制之后是一个前馈网络，它对每个位置应用相同的全连接层。
5. 前馈网络的输出也通过一个残差连接，随后是层归一化；

## Property
- Transformer-Encoder的这种设计使其能够并行处理数据，同时捕获序列内的复杂关系；
- 编码器的最终输出捕捉了输入序列中每个元素周围的上下文，可以看作是整个输入序列的编码；
- 在解码过程中，编码器的输出被用作解码器的自注意力层中的Key和Value，这样解码器就能够利用输入序列的上下文信息来生成目标序列；


# Transformer-Decoder

## Structure
Decoder是由多个一样的结构堆叠而成，每个结构由下列模块组成：
1. 遮掩多头自注意力机制（Masked Multi-Head Self-Attention）： 在解码器的自注意力层中，由于输出序列是逐项生成的，为了防止位置$i$的解码器看到位置$i$之后的数据（这些数据在实际预测时是未知的），需要使用一个遮掩（mask）来屏蔽未来的位置信息。这样，每个位置只能依赖于它之前的位置，保证了解码过程的自回归特性(每一步只依赖于当前时间步之前已经生成的所有输出，而不会直接使用未来的输出)；
2. 编码器-解码器注意力机制（Multi-Head Cross-Attention）： 这个子结构使得解码器能够关注（attend to）编码器的输出。在这个注意力机制中，$Q$来自于解码器的前一个子层的输出，而$K$和$V$来自于编码器的输出。这允许解码器的每个位置都能够访问整个输入序列的信息；
3. 位置全连接的前馈网络（Positionwise Feed-Forward Network）： 与编码器中的前馈网络类似，这个网络由两个线性变换组成，其中间插入了一个激活函数；

## Property
- 生成输出序列：基于编码器提供的输入序列信息以及到目前为止已经生成的输出序列部分，解码器逐步构造输出序列；
- 保持自回归属性：通过遮掩机制，确保在生成序列的每一步只依赖于前面的输出，从而使模型能够进行自回归预测；
- 值得注意的是，在有些应用场合，比如说基于Transformer的Object Detector（比如DETR）不是自回归的，也就是说，它不是逐步生成输出序列的每一个元素。相反，DETR的解码器在单个前向传递中并行生成所有输出；

# Ref
- [保姆级transformer解释](https://wangguisen.blog.csdn.net/article/details/125074022?spm=1001.2014.3001.5502)
- [Positional Encoding](https://www.zhihu.com/question/347678607)
- [Attention详解](https://blog.csdn.net/qq_42363032/article/details/124651978)

