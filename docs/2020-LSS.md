---
share: true
---

# Problem

- 自动驾驶的传感器各自有自己的坐标系，与车身坐标系不是一个坐标。现有的CV任务一般都是在自己的坐标系下进行的；
- 现有的自动驾驶一般只考虑一个输入View，无法在数据驱动的框架之下处理多视角的问题；

# Approach

## Lift: Latent Depth Distribution

- Input：Image，HxW
- Process：
	1. 对每个像素点配上D个深度值(depth-bin)，生成$H*W*D$个点组成的点云；值得注意的是这个点云并不是长方形的，而是视锥形的。详见论文的Fig.4 
	2. 对每个像素点网络预测两个东西：
		- 深度分布$\alpha$，深度可能的取值有D个，分配这个像素的D个点；所谓的深度分布是指这个像素可能的深度值的概率；
		- context vector：$\bf{c}$，这是一个长度为C的向量；
	3. 对于点云中的一个点，我们们可以通过它的像素位置和深度获得它的特征向量，$p_d=\alpha_d\bf{C}$；
- Output：
	1. $H*W*D$个点和他们每个点的context vector；

## Splat: Pillar Pooling
- Input: 上一步的输出的视锥形点云和他们的context vector：
- Process:
	1. 利用内外参把点云投影到BEV平面；这就是所谓的Unprojecting；
	2. 在BEV平面上把点分配到最近的PointPillar，并进行pooling；
	3. 在BEV上应用分割网络输出结果；
- output：BEV平面的分割结果

## Shooting
- 用来做Planning的，不太相关；

# Architecture

- Lift：EfficientNet-B0
- Splat：无神经网络
- 分割：ResNet Variant

#  Ref
- Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D

