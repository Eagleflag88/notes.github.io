---
share: true
---
# Ref
[各种卷积操作](https://zhuanlan.zhihu.com/p/70703846) 
[Weight Init](https://pouannes.github.io/blog/initialization/)

# Normalization

## Batch Normalization

### Concept
- Batch Normalization（批量归一化）是一种在深度神经网络中常用的技术，用以加速训练过程并提高模型的稳定性。其核心思想是在网络的每个批量数据通过激活函数之前，先对该层的输入进行归一化处理，即将输入数据调整为均值为0、方差为1的分布；
- 这样做可以减少所谓的“内部协变量偏移”（internal covariate shift），并允许使用更高的学习率以及减少模型对初始化权重的敏感度；

### Process
BN在每个mini-batch的数据通过以下步骤进行：

1. 计算均值：计算当前mini-batch的均值。
2. 计算方差：计算当前mini-batch的方差；
3. 这里的计算范围是特征图的某一个通道，比如说特征图的维度是50x60x8，那么均值是在某一个通道的特征图层内计算的，即是计算比如说50x60个数的均值和方差；如果batch_size=8，则是计算50x60x8个数的均值和方差；
4. 归一化：使用上述均值和方差对输入数据进行归一化处理，确保输出数据的均值为0，方差为1；
5. 缩放和偏移：对归一化后的数据进行缩放和偏移变换，这里引入了两个可学习参数，即γ（缩放因子）和β（偏移因子）；

In Training using the mean and variance of the mini batches, at inference the mean and variance of the whole training set will be used.

### 可学习参数
Batch Normalization 有两组可学习参数：

1. γ（Gamma）: 缩放参数，用于对归一化后的数据进行缩放。
2. β（Beta）: 偏移参数，用于对归一化后的数据进行平移。
3. 每个通道会有一组参数；

## Batch ReNormalization

Batch ReNormalization 是 Batch Normalization 的一个变种，它旨在解决 Batch Normalization 在小批量数据上应用时效果不佳的问题。Batch ReNormalization 引入了两个新的参数，用于调整每个 mini-batch 的均值和方差，以便更好地近似整个数据集的统计特性。这使得它在使用小批量或非独立同分布（non-i.i.d.）数据时更为稳健。在 Batch ReNormalization 中，mini-batch 的均值和方差会被调整以考虑整个数据集的均值和方差，从而减少在训练不同阶段由于批量大小不一致引起的内部协变量偏移。


## Group Normalization

GN是一种在神经网络中使用的方法，尤其是在深度学习模型中，用来稳定学习过程。它是BN的替代方法，特别适用于小批量尺寸的训练。

BN是在批量维度上对特征图进行归一化，而GN通过将通道分成组，在这些组内计算用于归一化的均值和方差。这使得GN能够独立于批量大小操作，因此在由于内存限制而不能依赖大批量大小的情况下特别有优势。

GN背后的关键概念包括：

1. 通道分组：特征图的通道被划分为多个组。组的数量是一个可以调整的超参数。如果组的数量设置为通道数，GN则与实例归一化（Instance Normalization）类似。
2. 归一化：GN对每个组计算归一化的均值和方差。归一化是按通道进行的，它是对每个单独样本应用的，而不是跨批量。即使在batch size大于1的情况下也不会跨Batch；
3. 缩放和位移参数：归一化之后，GN应用可学习的缩放和位移参数来保持网络的表示能力。这些参数在训练过程中学习。
4. 独立于批量大小：由于GN在通道组内计算统计数据，它不需要大批量大小。这对于需要高分辨率输入图像的任务或当可用内存有限时特别有用。
5. 训练稳定性：GN可以导致深度学习模型的训练更加稳定，特别是在BN可能不适用的小批量大小或需要跨不同样本一致归一化的任务中。

GN在各种任务中得到应用，如图像识别、目标检测和语义分割，并且在批量大小受到硬件限制的场景中特别有益。

## Layer Normalization
详见[[Transformer#Layer Normalization|Transformer > Layer Normalization]]

# Loss Function

## Cross-Entropy Loss

交叉熵损失（Cross-Entropy Loss）是评估分类模型性能的一种常用方法，特别是在二分类和多分类问题中。它衡量的是模型预测的概率分布与实际标签的概率分布之间的差异。对于二分类问题，交叉熵损失可以定义为：

$$
L(y, p) = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
$$

其中：

- N 是样本数量；
- $y_i$ 是样本i的实际标签；
- $p_i$ 是模型预测样本 i 为类别 1 的概率；

在实际应用中，交叉熵损失函数通常与softmax激活函数一起使用，后者用于多分类问题的输出层，将模型的原始输出转换为概率分布。

## Softmax

交叉熵损失（Cross-Entropy Loss）后面通常会跟一个softmax操作，尤其是在处理多分类问题时。这样做的原因是交叉熵损失要求输入是概率分布，而模型的原始输出，通常称为logits，通常并不满足这一点。softmax函数将logits转换为概率分布，这样就可以与真实的标签概率分布进行比较了。

softmax函数的定义是：
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

这里，$z$ 是模型的logits向量，$z_i$​ 是该向量中的第i个元素。

数值计算：
- 问题1：
	1. 当$z_i$太大且大于0，直接计算$z_i$的指数会让计算Overflow；
	2. 当$z_i$是负数且很大，会让$z_i$的指数趋近于0。如果在分母发生会导致除零。如果在分子发生会导致取log之后输出负无穷；
- Approach：把$z_i$替换为$z_i-\max(z_i)$；

## Focal Loss
Focal Loss是一种专门为解决类别不平衡问题而设计的损失函数，由Kaiming He等人在2017年提出。它是交叉熵损失函数的一个变种，通过增加一个因子来减少易分类样本的相对损失，从而使模型更加关注难分类或者错误分类的样本。

Focal Loss的数学定义为：

$$
FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

其中：

- $p_t$​ 是模型对于实际标签 t 预测为正确类别的概率。
- $\alpha_t$​ 是正样本的权重，用于调节正负样本的重要性，可以是一个常数或者是类别的权重。
- $\gamma$是调节因子，称为聚焦参数（focusing parameter），它的作用是减少容易分类样本的损失的影响，并使模型更加关注难以分类的样本。

当 $\gamma=0$ 时，Focal Loss退化为标准的交叉熵损失；当 $\gamma>0$ 时，调节因子$(1-p_t)^\gamma$ 会降低易于分类样本的损失贡献，增加了对困难或错误分类样本的惩罚。

Focal Loss特别适合于处理那些在类别分布极度不平衡的情况下的目标检测问题，比如在一幅图像中背景类的样本远多于前景类的样本。它使得模型在训练过程中更加关注那些难以正确分类的样本，从而提高整体的模型性能。

## Smooth L1 Loss

平滑L1损失（Smooth L1 Loss）是一种广泛用于回归问题中的损失函数，特别是在目标检测的边界框回归任务中。这种损失函数结合了L1损失（绝对值损失）和L2损失（平方损失）的优点，目的是减小对异常点的敏感度，并在误差较小时提供更平滑的梯度。

平滑L1损失在数学上定义为：

$$
\begin{cases} 0.5 x^2 & \text{if } |x| < 1, \\ |x| - 0.5 & \text{otherwise.} \end{cases}
$$

- 当预测误差较小时，损失函数采用平方形式，使得梯度相对较小，从而对模型参数的更新更加平滑； 
- 当预测误差较大时，损失函数切换为线性形式，减少了对离群点的敏感性，因为不再对大误差进行平方放大，避免了L2损失在面对大误差时梯度过大的问题；

## IoU Loss

IoU损失（Intersection over Union Loss），又称为Jaccard损失，是一种在目标检测和图像分割任务中常用的损失函数，用于评估预测边界框或分割区域与真实值之间的重叠程度。IoU本身是一个度量标准，用于计算预测区域和实际区域的交集与它们的并集之比。

IoU损失是1减去IoU的值，用于在优化过程中最大化IoU度量：

$$
IoU \ Loss=1−IoU
$$


IoU损失鼓励模型增加预测区域与真实区域的重叠，从而提高预测的精确度。

### 可导性

然而，IoU损失在预测区域和真实区域没有交集的情况下不可导，因为在这种情况下，IoU的值为0，对预测边界框的参数的微小变化不会影响IoU的值，即梯度为0。这就导致了在没有重叠时，梯度更新不能通过IoU损失直接进行。

为了解决这个问题，研究人员提出了一些IoU损失的变体，如：

- Smooth IoU Loss：通过添加一个小常数来平滑损失函数，确保梯度的存在。
- Generalized IoU (GIoU) Loss：GIoU损失考虑了边界框之间的距离，即使没有重叠也能提供非零梯度。
- Distance-IoU (DIoU) 和 Complete IoU (CIoU) Loss：这些都是进一步的改进，它们不仅考虑了边界框之间的重叠度，还考虑了它们之间的距离和长宽比。

这些变体确保了即使在没有交集的情况下，损失函数也是可导的，并且可以更有效地指导模型学习。

总结来说，IoU损失及其变体是优化目标检测和图像分割模型中边界框定位的有效方法。它们通过对预测和真实边界框之间的重叠度进行量化，直接影响模型的训练过程，并解决了原始IoU损失在梯度方面的问题。

## Dice Loss

Dice coefficient measures the overlap between the predicted mask and the ground truth mask.
$$
Dice(A, B) = \frac{2 \cdot |A \cap B|}{|A| + |B|}
$$
The Dice loss is then calculated as the complement of the Dice coefficient
$$
DiceLoss(A, B) = 1 - Dice(A, B)
$$
Dice Loss本身不可导，一般用作Eval的指标。但是如果一定要用来训练的话，可以使用smoothed version


# Training

## Preprocessing

### Normalization
- Whitening：让数据的协方差矩阵为单位矩阵；
- PCA：让数据的协方差矩阵只有对角线元素非0；
- Best Practice：一般只会center；

### Dataset Partition
- 分为training, validation, test三个数据集；
- 利用training和validation做cross validation获得超参数；
- 最后在test集上跑一次，获得结果；

## Weight Initialization

权重初始化在深度学习模型的训练中非常关键，它可以显著影响模型的收敛速度和最终性能。以下是一些权重初始化的关键点：

1. 避免权重初始值过大或过小：权重过大可能导致神经元饱和，梯度消失或爆炸；权重过小也可能导致梯度消失。
2. 随机初始化：通常权重是随机初始化的，以打破对称性并确保不同的神经元可以学习不同的特征。
3. 基于输入和输出尺寸的初始化（如Xavier/Glorot初始化）：适用于tanh激活函数，初始化时考虑到前一层的神经元数量，以保持激活和梯度的分布稳定。它的结果是一个以0为均值的高斯分布，$D_{in}$是某一层的输入维度：
	$$
	W = \mathcal{N}(0, \sqrt{\frac{1}{D_{in}}})
  $$
4. He初始化：特别为ReLU激活函数设计，考虑到ReLU在负值区域的神经元不活跃的特性，因此初始权重需要更大。它的结果是：
	$$
	W = \mathcal{N}(0, \sqrt{\frac{2}{D_{in}}})
  $$
5. 稀疏初始化：只为每个神经元设置少量的非零权重，可以帮助减少早期训练中的过拟合风险。
6. 正交初始化：对于循环神经网络（RNN）特别有用，可以帮助减少梯度消失或爆炸的问题。
7. 使用预训练权重：在可能的情况下，可以使用在相似任务上训练过的模型权重，这种迁移学习方法可以加速学习过程并提高模型性能。

权重初始化的选择应考虑到所使用的激活函数、网络架构和任务的特性。正确的初始化方法可以确保梯度的良好流动，防止训练初期的梯度问题，并有助于模型更快地收敛。

## Regularization

在深度学习训练中，正则化技术用于防止模型过拟合，从而提高模型在新数据上的泛化能力。以下是一些常用的正则化技术：

1. 权重衰减（L2正则化）：又叫Weight Decay。通过在损失函数中添加一个与权重大小成比例的项来惩罚大的权重值，通常表现为所有权重的平方和乘以一个常数。
2. L1正则化：与L2类似，但是惩罚项是权重的绝对值之和，这可以导致稀疏的权重矩阵，有些权重会变成0。  
3. Dropout：在训练过程中随机丢弃（即设置为零）网络中的一些神经元输出，以减少神经元之间复杂的共适应。
	- 在训练的每次迭代中，每个神经元都有一定概率（通常设为 0.5）被随机丢弃，即它在前向传播和反向传播时暂时不参与计算；
	- 实际上等同于每次都在训练不同的网络；
	- 由于在训练时某些神经元被丢弃，因此在预测时需要对神经元的输出进行缩放，以补偿那些在训练时未被激活的神经元。如果在训练时使用了p的丢弃概率，则在预测时，网络的权重通常会乘以1−p（这个过程有时也被称为"inverted dropout"）。
1. 数据增强：通过对训练数据进行旋转、缩放、裁剪等变换来人为增加样本多样性，提高模型的泛化能力。
2. 早停（Early Stopping）：在验证集上的性能不再提升时停止训练，防止过拟合。
3. BN：通过对每个小批量的数据进行归一化，减少内部协变量偏移，可以作为一种正则化效果。详见[[Deep Learning Basics#Batch Normalization|Deep Learning Basics > Batch Normalization]]
4. LayerNorm：与BN类似，但是在单个样本的所有激活上进行归一化，与批大小无关。详见[[Transformer#Layer Normalization|Transformer > Layer Normalization]]
5. Group Normalization：一种中间方案，它在LayerNorm和BN之间提供一个折中方案，通过对通道分组来进行归一化。详见[[Deep Learning Basics#Group Normalization|Deep Learning Basics > Group Normalization]]
6. 梯度剪切（Gradient Clipping）：限制梯度更新的步长，防止梯度爆炸。
7. 学习率衰减：随着时间的推移逐渐减小学习率，有助于模型在训练后期稳定。

这些技术可以单独使用，也可以组合使用，以达到最好的正则化效果。选择哪种技术通常取决于具体任务、模型复杂性和训练数据的特性。

## Optimization

### 一阶优化算法
- Gradient Steepest：
	1. 直接朝gradient的反方向走；
	2. “最陡”方向可以根据不同的范数来定义。例如，在欧几里得范数下，最陡下降方向与梯度下降相同；
- Gradient Descent：
	1. Zig-Zag Behavior because of poor conditioning；
	2. Get stuck in local minima or saddle points；
-  Stochastic Gradient Descent（SGD）：
	1. 小范围应用Batch Gradient Descent；
	2. 超参数batch size，一般选32；
-  SGD+Momentum：
	1. Get over with saddle points with velocity (initial 0)；
	2. Nesterov Momentum；
- AdaGrad：
	1. 缓解各方向梯度不均匀的问题；
	2. Approach：主要特点是在训练过程中累积了一个参数的历史梯度的平方和，然后用这个信息来调整每个参数的学习率；
	3. Issue：时间久了，增量容易变为0；
- RMSProp-AdaGrad：添加一个decay rate来缓解增量变0的问题；
- Adam：Combination of SGD-Momentum and AdaGrad；

## 	Learning Rate

在深度学习训练中，学习率是控制模型权重调整速度的一个关键超参数。学习率过高可能导致训练不稳定，甚至发散；学习率过低则可能导致训练速度过慢，或陷入局部最小值。因此，正确调整学习率对于有效训练深度神经网络至关重要。以下是一些调整学习率的常用策略：

1. 预设减小学习率：事先设定一个学习率调整计划，随着训练的进行逐步降低学习率。例如，可以每过一定数量的epochs将学习率减小一半。
2. 性能调度：观察模型在验证集上的性能，如果性能停止提高，则降低学习率。
3. 指数或者步进衰减：按照指数函数逐渐减小学习率，或者在固定的训练步数后降低学习率。
4. 适应性学习率算法：例如Adagrad, RMSprop, 和Adam等，这些算法能够自适应地调整每个参数的学习率，通常不需要手动调整。
5. 周期性调整：如余弦退火学习率（Cosine Annealing），在每个周期内逐渐降低学习率，然后重置到较高的值。
6. 热启动重启：这是周期性调整的一种变体，每次重启时将学习率设置到较大的值，允许模型跳出局部最小值。    

调整学习率的最佳方法可能依赖于具体的模型、数据集和训练设置。在实践中，通常需要结合使用上述几种策略，并可能需要多次尝试和交叉验证来确定最优的学习率调整策略。


## 连乘效应
在神经网络的反向传播过程中，连乘效应指的是梯度在多层网络中传递时的累积乘积。由于深度学习模型通常由多层组成，梯度必须通过这些层反向传播以更新权重。每一层的梯度是由该层的激活函数导数和来自上一层的梯度的乘积计算得出的。这个连乘过程可以导致两个主要问题：梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Exploding）。

### 梯度消失：

当网络很深时，梯度在经过多个层的反向传播过程中可能会越来越小，最终变得非常微小，使得权重更新非常缓慢或者几乎不更新。这种现象称为梯度消失。尤其是当使用像Sigmoid或Tanh这样的饱和激活函数时，它们的导数在输入值非常大或非常小的时候会接近于零，导致梯度消失问题更加严重。

### 梯度爆炸：

与梯度消失相反，梯度爆炸是指梯度在经过多个层的反向传播时变得越来越大。这会导致权重更新过大，使得网络权重波动非常剧烈，甚至导致模型发散。在实际操作中，梯度爆炸通常通过梯度剪切（Gradient Clipping）来缓解。

### 解决方法：

1. 非饱和激活函数：使用像ReLU及其变体这样的非饱和激活函数可以缓解梯度消失问题。
2. 合适的权重初始化：如He初始化或Xavier初始化，它们可以帮助在训练初期保持梯度的规模。
3. 批量归一化（Batch Normalization）：可以减少训练过程中各层输入分布的改变，有助于缓解梯度消失问题。
4. 残差连接（Residual Connections）：如在ResNet中使用的跳过连接，允许梯度直接流向前面的层，可以减轻梯度消失。
5. 梯度剪切：在反向传播过程中限制梯度的最大值，以防止梯度爆炸。

连乘效应是深度神经网络设计和训练中需要特别注意的问题，正确理解和处理这个问题对于训练稳定和有效的模型至关重要。

# Evaluation

## Classfication
- Top1 Accuracy: 通常指模型预测的最高置信度类别是否与真实类别匹配的准确性；
- Top5 Accuracy: 指的是模型预测的前五个最高置信度的类别中是否包含了真实的类别。换句话说，如果真实的类别出现在模型预测的置信度最高的前五个类别中的任何一个，那么这个预测就被认为是正确的;






